{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NNFramework_fromScratch.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSoCd4YODaDP"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib as plt\n",
        "import sys"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5alSD2__CvN7"
      },
      "source": [
        "# The Neuron"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i0m88rrnEBu7",
        "outputId": "df7e8bf3-a063-4280-fec0-192b183939a8"
      },
      "source": [
        "X = [ # init inputs of batch size 3 from 4 seperate neurons (3,4)\n",
        "          [1,2,3,2.5],\n",
        "          [2,5,-1,2],\n",
        "          [-1.5,2.7,3.3,-.8],\n",
        "          ]\n",
        "np.random.seed(0)\n",
        "# dense layer class\n",
        "class layer_dense:\n",
        "  def __init__(self, n_inputs, n_neurons):\n",
        "    # randomly initialize weights and biases\n",
        "    self.weights = .10 * np.random.randn(n_inputs, n_neurons)\n",
        "    self.biases = np.zeros((1, n_neurons))\n",
        "  def forward(self, inputs):\n",
        "    # output inputs dot weights + bias for each individual neuron\n",
        "    self.output = np.dot(inputs, self.weights) + self.biases\n",
        "\n",
        "class relu_activation:\n",
        "  def forward(self, inputs):\n",
        "    # relu: if input is < 0, return 0, if larger, return input\n",
        "    # this is used to delinearize neural networks, neural networks CANNOT fit\n",
        "    # an unknown NONLINEAR function with completely linear layers, however with non-linear\n",
        "    # activations, neural networks have been proven to be able to fit ANY function, linear or not\n",
        "    self.output = np.maximum(0, inputs)\n",
        "class softmax_activation:\n",
        "  def forward(self, inputs):\n",
        "    # softmax is an exponentiation then normalization of an input\n",
        "    # this is important because with just a relu or linear function\n",
        "    # certain problems arise. ie. with a relu function, negatives become\n",
        "    # clipped to zero, so in a classification network, one output could be\n",
        "    # -.99 and the other be -.1. A relu would output both as 0, so how can\n",
        "    # some sort of post processing step get the correct prediction? or even\n",
        "    # worse, how could the neural network run back propagation? the loss function\n",
        "    # would be just as sensitive to both clipped inputs.\n",
        "    exponentiate_input = np.exp(inputs - np.max(inputs, axis = 1, keepdims = True))\n",
        "    normalize_input = exponentiate_input / np.sum(exponentiate_input, axis 1, keepdims = True))\n",
        "    self.output = probabilities\n",
        "class loss_function\n",
        "  def calculate_loss(self, outputs, y):\n",
        "    losses = self.forward(output, y)\n",
        "    batch_loss = np.mean(losses)\n",
        "    return batch_loss\n",
        "\n",
        "class categorical_crossentropy(loss_function):\n",
        "  def forward(self, y_pred, y_hat):\n",
        "    samples = len(y_pred)\n",
        "    clipped_pred = np.clip(y_pred, 1e-7, 1-1e-7)\n",
        "    "
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.10758131 1.03983522 0.24462411 0.31821498 0.18851053]\n",
            " [0.         0.70846411 0.00293357 0.44701525 0.36360538]\n",
            " [0.         0.55688422 0.07987797 0.         0.04553042]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vbJXwWIjBp_v"
      },
      "source": [
        "# Activation functions\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}